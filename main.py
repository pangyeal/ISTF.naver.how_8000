# main.py --reload --port 8000
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import yt_dlp
import os
import re
from openai import OpenAI
from typing import Optional
import json
import argparse
from config import Config
from database import Database

import requests

def call_claude(system_prompt, user_prompt):
    headers = {
        "Authorization": f"Bearer {load_openai_api_key()}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "anthropic/claude-3-opus-20240229",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "max_tokens": 256  # ë¬´ë£Œ ì‚¬ìš©ì ê¸°ì¤€ ì ì ˆí•œ í† í° ì œí•œ
    }

    response = requests.post(
        "https://openrouter.ai/api/v1/chat/completions",
        headers=headers,
        json=payload
    )

    if response.status_code != 200:
        # ì‘ë‹µì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° detail ë©”ì‹œì§€ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        try:
            err = response.json().get("error", {}).get("message", "Claude í˜¸ì¶œ ì‹¤íŒ¨")
        except Exception:
            err = "Claude ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜"
        raise Exception(f"Claude API ì˜¤ë¥˜: {err}")

    result = response.json()
    if "choices" not in result:
        raise Exception(f"Claude ì‘ë‹µ ì˜¤ë¥˜: choices í•„ë“œ ì—†ìŒ â†’ {result}")

    return result["choices"][0]["message"]["content"]


# ì•± ì´ˆê¸°í™” ì „ì— ì„¤ì • ê²€ì¦
Config.init_app()

app = FastAPI()
app.mount("/static", StaticFiles(directory=Config.STATIC_DIR), name="static")
templates = Jinja2Templates(directory=Config.TEMPLATE_DIR)

# ì „ì—­ ë°ì´í„°ë² ì´ìŠ¤ ê°ì²´ë¥¼ Noneìœ¼ë¡œ ì´ˆê¸°í™”
db = None


def detect_language(text):
    # ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€: í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ í•œêµ­ì–´ë¡œ íŒë‹¨
    if any(ord('ê°€') <= ord(char) <= ord('í£') for char in text):
        return 'ko'
    # ì—¬ê¸°ì— í•„ìš”í•œ ë‹¤ë¥¸ ì–¸ì–´ ê°ì§€ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
    return 'en'

def get_db():
    global db
    if db is None:
        import sys
        # uvicorn ì‹¤í–‰ ì‹œ ëª…ë ¹í–‰ ì¸ìì—ì„œ í¬íŠ¸ ì°¾ê¸°
        port = Config.DEFAULT_PORT
        for i, arg in enumerate(sys.argv):
            if arg == "--port" and i + 1 < len(sys.argv):
                port = int(sys.argv[i + 1])
                break
        db = Database(Config.DATABASE_URL, port)
    return db


# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
def load_openai_api_key():
    try:
        with open("openaisec.key", "r") as key_file:
            return key_file.read().strip()
    except FileNotFoundError:
        raise Exception("API í‚¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


client = OpenAI(api_key=load_openai_api_key())


class YouTubeURL(BaseModel):
    url: str


def get_next_file_number():
    existing_files = [f for f in os.listdir(Config.DOWNLOAD_PATH)
                      if f.startswith('1') and f.endswith('.vtt')]
    if not existing_files:
        return 1000
    numbers = [int(re.search(r'(\d+)', f).group(1))
               for f in existing_files if re.search(r'(\d+)', f)]
    return max(numbers) + 1 if numbers else 1000


def extract_text_from_vtt(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    text_lines = []
    for line in lines:
        if "-->" in line:
            continue
        clean_line = re.sub(r"<[^>]*>", "", line).strip()
        if clean_line and not clean_line.startswith('WEBVTT'):
            text_lines.append(clean_line)

    unique_lines = []
    seen = set()
    for line in text_lines:
        if line not in seen:
            unique_lines.append(line)
            seen.add(line)

    return "\n".join(unique_lines)


@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    db_instance = get_db()
    remaining_count = db_instance.get_remaining_count()
    return templates.TemplateResponse(
        "index.html",
        {"request": request, "remaining_count": remaining_count}
    )


@app.get("/get_count")
async def get_count():
    db_instance = get_db()
    return {"count": db_instance.get_remaining_count()}


def get_prompts(detected_lang, extracted_text):
    system_prompts = {
        'ko': (
            "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì„¸ ê°€ì§€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤:\n"
            "1. í…ìŠ¤íŠ¸ì˜ ì „ì²´ì ì¸ ë‚´ìš©ì„ 3~5ë¬¸ì¥ìœ¼ë¡œ ê°„ëµíˆ ìš”ì•½í•©ë‹ˆë‹¤.\n"
            "2. í…ìŠ¤íŠ¸ë¥¼ ì£¼ì œë³„ë¡œ ë‚˜ëˆ„ì–´ ëª©ì°¨ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
            "3. ê° ëª©ì°¨ í•­ëª©ë³„ë¡œ ì£¼ìš” ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.\n\n"
            "ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì œê³µí•˜ì„¸ìš”:\n\n"
            "ì „ì²´ ìš”ì•½:\n[ì „ì²´ ë‚´ìš©ì„ 3~5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½]\n\n"
            "ëª©ì°¨ ë° ì„¸ë¶€ ë‚´ìš©:\n"
            "1. [ì£¼ì œ1]\n"
            "   - [í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ 2~3ë¬¸ì¥ ì„¤ëª…]\n"
            "2. [ì£¼ì œ2]\n"
            "   - [í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ 2~3ë¬¸ì¥ ì„¤ëª…]\n"
            "[ì´í•˜ ê³„ì†...]"
        ),
        'en': (
            "You are an expert at performing three levels of text analysis:\n"
            "1. Provide a concise overall summary in 3-5 sentences.\n"
            "2. Structure the content into a clear outline of main topics.\n"
            "3. For each topic, provide 2-3 sentences of detailed explanation.\n\n"
            "Your output should follow this format:\n\n"
            "Overall Summary:\n[3-5 sentence summary of the entire content]\n\n"
            "Detailed Outline:\n"
            "1. [Topic 1]\n"
            "   - [2-3 sentences explaining this topic]\n"
            "2. [Topic 2]\n"
            "   - [2-3 sentences explaining this topic]\n"
            "[continue as needed...]"
        )
    }

    user_prompts = {
        'ko': (
            f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì „ì²´ ìš”ì•½ê³¼ í•¨ê»˜ ê° ì£¼ì œë³„ ìƒì„¸ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”. "
            f"íŠ¹íˆ ê° ì£¼ì œë³„ë¡œ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”.\n\n"
            f"í…ìŠ¤íŠ¸:\n{extracted_text}"
        ),
        'en': (
            f"Please analyze the following text, providing both an overall summary and detailed explanations for each topic. "
            f"Include specific examples and important details for each section.\n\n"
            f"Text:\n{extracted_text}"
        )
    }

    return system_prompts.get(detected_lang, system_prompts['en']), user_prompts.get(detected_lang, user_prompts['en'])


@app.post("/process_url")
async def process_url(youtube_url: YouTubeURL):

    db_instance = get_db()
    if not db_instance.decrease_count():
        raise HTTPException(status_code=403,
                            detail="ì‚¬ìš© ê°€ëŠ¥í•œ ìš”ì²­ ìˆ˜ê°€ ëª¨ë‘ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.")

    try:
        file_number = get_next_file_number()
        base_filename = f"{file_number}"

        ydl_opts = {
            'writesubtitles': True,
            'writeautomaticsub': True,
            'subtitleslangs': ['ko'],
            'subtitlesformat': 'vtt',
            'skip_download': True,
            'outtmpl': os.path.join(Config.DOWNLOAD_PATH, base_filename),
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info_dict = ydl.extract_info(youtube_url.url, download=True)

        vtt_file_path = os.path.join(Config.DOWNLOAD_PATH, f"{base_filename}.ko.vtt")
        extracted_text = extract_text_from_vtt(vtt_file_path)

                
        # âœ… ë¬´ë£Œ ì‚¬ìš©ì í† í° ì´ˆê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ìë§‰ ìë¥´ê¸°
        # ë„ˆë¬´ ê¸¸ ê²½ìš° Claude í˜¸ì¶œì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ 500ì ì œí•œ ê¶Œì¥
        MAX_TRANSCRIPT_CHARS = 500
        
        if len(extracted_text) > MAX_TRANSCRIPT_CHARS:
            extracted_text = extracted_text[:MAX_TRANSCRIPT_CHARS]

        # ğŸ’¡ ë§Œì•½ ìœ ë£Œ API ì‚¬ìš©ìë¼ë©´ ì•„ë˜ ìë¥´ê¸° ì½”ë“œë¥¼ ì£¼ì„ ì²˜ë¦¬í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
        # Claude Opus ë˜ëŠ” GPT-4-turboëŠ” ë” ê¸´ ìë§‰ë„ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.

        # ì–¸ì–´ ê°ì§€
        detected_lang = detect_language(extracted_text)

        # process_url í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•  ë•Œ:
        system_prompt, user_prompt = get_prompts(detected_lang, extracted_text)

        # response = client.chat.completions.create(
        #     model="gpt-4-1106-preview",
        #     messages=[
        #         {"role": "system", "content": system_prompt},
        #         {"role": "user", "content": user_prompt}
        #         # {"role": "user",
        #         #  "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{extracted_text}" if detected_lang == 'ko' else f"Summarize the following text:\n\n{extracted_text}"}
        #     ]
        # )

        summary = call_claude(system_prompt, user_prompt)

        return {
            "success": True,
            "transcript": extracted_text,
            "summary": summary,
            "remaining_count": db_instance.get_remaining_count()
        }

    except Exception as e:
        db_instance.increase_count()  # ì‹¤íŒ¨ ì‹œ ì¹´ìš´íŠ¸ ë³µêµ¬
        raise HTTPException(status_code=500, detail=str(e))

def main():
    parser = argparse.ArgumentParser(description='YouTube Subtitle Downloader Service')
    parser.add_argument('--port', type=int, default=Config.DEFAULT_PORT,
                        help=f'Port number to run the service on (range: {Config.PORT_RANGE.start}-{Config.PORT_RANGE.stop - 1})')
    args = parser.parse_args()

    try:
        port = Config.validate_port(args.port)

        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        global db
        db = Database(Config.DATABASE_URL, port)

        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=port)
    except ValueError as e:
        print(f"Error: {e}")
        exit(1)


if __name__ == "__main__":
    main()